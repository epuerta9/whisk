{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install llama-index nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisk.kitchenai_sdk.kitchenai import KitchenAIApp\n",
    "from whisk.kitchenai_sdk.http_schema import (\n",
    "    ChatCompletionRequest,\n",
    "    ChatCompletionResponse,\n",
    "    ChatCompletionChoice,\n",
    "    ChatResponseMessage\n",
    ")\n",
    "\n",
    "# Initialize the app\n",
    "kitchen = KitchenAIApp(namespace=\"whisk-jupyter-app-modified\")\n",
    "\n",
    "@kitchen.chat.handler(\"chat.completions,new\")\n",
    "async def handle_chat(request: ChatCompletionRequest) -> ChatCompletionResponse:\n",
    "    \"\"\"Simple chat handler that echoes back the last message\"\"\"\n",
    "\n",
    "\n",
    "    print(request)\n",
    "    return ChatCompletionResponse(\n",
    "        model=request.model,\n",
    "        choices=[\n",
    "            ChatCompletionChoice(\n",
    "                index=0,\n",
    "                message=ChatResponseMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=f\"Echo: {request.messages[-1].content}\"\n",
    "                ),\n",
    "                finish_reason=\"stop\"\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisk.kitchenai_sdk.schema import (\n",
    "    ChatInput, \n",
    "    ChatResponse,\n",
    "    DependencyType,\n",
    "    SourceNode\n",
    ")\n",
    "\n",
    "@kitchen.chat.handler(\"chat.rag\", DependencyType.VECTOR_STORE, DependencyType.LLM)\n",
    "async def rag_handler(chat: ChatInput, vector_store, llm) -> ChatResponse:\n",
    "    \"\"\"RAG-enabled chat handler\"\"\"\n",
    "    # Get the user's question\n",
    "    question = chat.messages[-1].content\n",
    "    \n",
    "    # Search for relevant documents\n",
    "    retriever = vector_store.as_retriever(similarity_top_k=2)\n",
    "    nodes = retriever.retrieve(question)\n",
    "    \n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\".join(node.node.text for node in nodes)\n",
    "    prompt = f\"\"\"Answer based on context: {context}\\nQuestion: {question}\"\"\"\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = await llm.acomplete(prompt)\n",
    "    \n",
    "    # Return response with sources\n",
    "    return ChatResponse(\n",
    "        content=response.text,\n",
    "        sources=[\n",
    "            SourceNode(\n",
    "                text=node.node.text,\n",
    "                metadata=node.node.metadata,\n",
    "                score=node.score\n",
    "            ) for node in nodes\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisk.kitchenai_sdk.schema import (\n",
    "    WhiskStorageSchema,\n",
    "    WhiskStorageResponseSchema\n",
    ")\n",
    "import time\n",
    "\n",
    "@kitchen.storage.handler(\"storage\")\n",
    "async def storage_handler(data: WhiskStorageSchema) -> WhiskStorageResponseSchema:\n",
    "    \"\"\"Storage handler for document ingestion\"\"\"\n",
    "    if data.action == \"list\":\n",
    "        return WhiskStorageResponseSchema(\n",
    "            id=int(time.time()),\n",
    "            name=\"list\",\n",
    "            files=[]\n",
    "        )\n",
    "        \n",
    "    if data.action == \"upload\":\n",
    "        return WhiskStorageResponseSchema(\n",
    "            id=int(time.time()),\n",
    "            name=data.filename,\n",
    "            label=data.model.split('/')[-1],\n",
    "            metadata={\n",
    "                \"namespace\": data.model.split('/')[0],\n",
    "                \"model\": data.model\n",
    "            },\n",
    "            created_at=int(time.time())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kitchen app: whisk-jupyter-app-modified version: 0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [2390539]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:42454 - \"OPTIONS /v1/models HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 10:58:22,979 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:42456 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59270 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 10:58:40,996 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [2390539]\n"
     ]
    }
   ],
   "source": [
    "from whisk.config import WhiskConfig, ServerConfig\n",
    "from whisk.router import WhiskRouter\n",
    "import asyncio\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configure and create router\n",
    "config = WhiskConfig(server=ServerConfig(type=\"fastapi\"))\n",
    "router = WhiskRouter(kitchen_app=kitchen, config=config)\n",
    "\n",
    "# Run the server\n",
    "\n",
    "router.run(host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kitchenai-whisk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
